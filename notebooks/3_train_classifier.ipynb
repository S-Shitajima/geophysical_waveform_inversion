{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, StochasticWeightAveraging, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchmetrics.functional.classification import multiclass_confusion_matrix, multiclass_f1_score\n",
    "\n",
    "from configs.config import CFG\n",
    "from util.my_dataset import MyDataModule, MyDataset\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ee4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitClassifierModel(L.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int,\n",
    "            learning_rate: float,\n",
    "        ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.model = timm.create_model(\"efficientnet_b2\", pretrained=True, in_chans=5, num_classes=0)\n",
    "        self.model.classifier = nn.Linear(self.model.num_features, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.save_hyperparameters(ignore=['criterion'])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        logit = self.model(x)\n",
    "        return logit\n",
    "\n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
    "        x, _, label = batch\n",
    "        logit = self.forward(x)\n",
    "        loss = self.criterion(logit, label)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
    "        x, _, label = batch\n",
    "        logit = self.forward(x)\n",
    "        loss = self.criterion(logit, label)\n",
    "        f1 = multiclass_f1_score(logit, label, num_classes=self.num_classes, average=\"macro\")\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_f1\", f1, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_test_epoch_start(self):\n",
    "        self.targets = []\n",
    "        self.preds = []\n",
    "\n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
    "        x, _, label = batch\n",
    "        logit = self.forward(x)\n",
    "        self.targets.append(label.cpu())\n",
    "        self.preds.append(logit.argmax(dim=1).cpu())\n",
    "        loss = self.criterion(logit, label)\n",
    "        f1 = multiclass_f1_score(logit, label, num_classes=self.num_classes, average=\"macro\")\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_f1\", f1, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"f1\": f1}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        targets = torch.cat(self.targets)\n",
    "        preds = torch.cat(self.preds)\n",
    "        cm = multiclass_confusion_matrix(preds, targets, num_classes=self.num_classes)\n",
    "        print(cm)\n",
    "        del self.targets\n",
    "        del self.preds\n",
    "    \n",
    "    def predict_step(self, x: torch.Tensor):\n",
    "        logit = self.forward(x)\n",
    "        return logit\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optimizer,\n",
    "            max_lr=self.learning_rate,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=0.3,\n",
    "            div_factor=25,\n",
    "            final_div_factor=1e+04,\n",
    "        )\n",
    "        # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.trainer.estimated_stepping_batches, eta_min=1e-5)\n",
    "        scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"strict\": False,\n",
    "        }\n",
    "        return (\n",
    "            {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": scheduler_config,\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329b68f",
   "metadata": {},
   "source": [
    "### Define Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceeb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_time = datetime.datetime.now()\n",
    "output_dir = Path(f\"../output/classifier_{now_time.date()}-{now_time.hour:02}-{now_time.minute:02}\")\n",
    "\n",
    "config = CFG(\n",
    "    output_dir=output_dir,\n",
    "    debag=False 3,\n",
    "    train_ratio=0.8,\n",
    "    seed=42,\n",
    "    batch_size=256,\n",
    "    epochs=10,\n",
    "    patience=5,\n",
    ")\n",
    "config.seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406304b",
   "metadata": {},
   "source": [
    "### Load Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4072e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = Path(\"../data\")\n",
    "print([p.stem for p in dir_path.glob(\"*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "families = {\n",
    "    \"CurveFault_A\": 0,\n",
    "    \"CurveFault_B\": 0,\n",
    "    \"CurveVel_A\": 1,\n",
    "    \"CurveVel_B\": 1,\n",
    "    \"FlatFault_A\": 2,\n",
    "    \"FlatFault_B\": 2,\n",
    "    \"FlatVel_A\": 3,\n",
    "    \"FlatVel_B\": 3,\n",
    "    \"Style_A\": 4,\n",
    "    \"Style_B\": 4, \n",
    "}\n",
    "\n",
    "paths = []\n",
    "for family, label in families.items():\n",
    "    for i, p in enumerate(dir_path.joinpath(family).glob(\"*.npz\")):\n",
    "        paths.append((family, label, p))\n",
    "        if config.debag and i == 1000:\n",
    "            break\n",
    "paths = pd.DataFrame(paths, columns=[\"family\", \"label\", \"path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9a2fc",
   "metadata": {},
   "source": [
    "### Split Paths into training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f71f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_paths, test_paths = train_test_split(\n",
    "    paths,\n",
    "    train_size=config.train_ratio,\n",
    "    shuffle=True,\n",
    "    random_state=config.seed,\n",
    "    stratify=paths[\"family\"]\n",
    ")\n",
    "train_paths, valid_paths = train_test_split(\n",
    "    train_valid_paths,\n",
    "    train_size=config.train_ratio,\n",
    "    shuffle=True,\n",
    "    random_state=config.seed,\n",
    "    stratify=train_valid_paths[\"family\"]\n",
    ")\n",
    "display(train_paths)\n",
    "display(valid_paths)\n",
    "display(test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1205e90",
   "metadata": {},
   "source": [
    "### Define The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitClassifierModel(\n",
    "    num_classes=5,\n",
    "    learning_rate=1e-03,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd7d8b",
   "metadata": {},
   "source": [
    "### Start Training with Dataset A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/statistics_A.pkl\", \"rb\") as f:\n",
    "    statistics_A = pickle.load(f)\n",
    "print(statistics_A[\"All\"][\"mean_x\"])\n",
    "print(statistics_A[\"All\"][\"std_x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd68131",
   "metadata": {},
   "outputs": [],
   "source": [
    "families_A = [\n",
    "    \"CurveFault_A\",\n",
    "    \"CurveVel_A\",\n",
    "    \"FlatFault_A\",\n",
    "    \"FlatVel_A\",\n",
    "    \"Style_A\",\n",
    "]\n",
    "\n",
    "train_paths_A = train_paths.query(\"family in @families_A\")\n",
    "valid_paths_A = valid_paths.query(\"family in @families_A\")\n",
    "test_paths_A = test_paths.query(\"family in @families_A\")\n",
    "display(train_paths_A)\n",
    "display(valid_paths_A)\n",
    "display(test_paths_A)\n",
    "display(pd.crosstab(train_paths_A[\"family\"], train_paths_A[\"label\"]))\n",
    "display(pd.crosstab(valid_paths_A[\"family\"], valid_paths_A[\"label\"]))\n",
    "display(pd.crosstab(test_paths_A[\"family\"], test_paths_A[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53820bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "datamodule_A = MyDataModule(\n",
    "    train_paths=train_paths_A,\n",
    "    valid_paths=valid_paths_A,\n",
    "    test_paths=test_paths_A,\n",
    "    seed=config.seed,\n",
    "    batch_size=config.batch_size,\n",
    "    mean_x=statistics_A[\"All\"][\"mean_x\"],\n",
    "    std_x=statistics_A[\"All\"][\"std_x\"],\n",
    "    mean_y=None,\n",
    "    std_y=None,\n",
    ")\n",
    "\n",
    "callbacks=[\n",
    "    EarlyStopping(monitor=\"val_f1\", patience=config.patience, mode='max'),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "    TQDMProgressBar(),\n",
    "    StochasticWeightAveraging(\n",
    "        swa_lrs=1e-5,\n",
    "        swa_epoch_start=int(0.8*config.epochs),\n",
    "        annealing_epochs=int(0.2*config.epochs),\n",
    "    ),\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=config.output_dir,\n",
    "    enable_checkpointing=False,\n",
    "    accelerator=\"cuda\",\n",
    "    max_epochs=config.epochs,\n",
    "    precision=\"bf16-mixed\",\n",
    "    callbacks=callbacks,\n",
    "    logger=CSVLogger(config.output_dir, name=\"classifier_A\"),\n",
    "    log_every_n_steps=1,\n",
    "    val_check_interval=None,\n",
    "    check_val_every_n_epoch=1,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule_A)\n",
    "trainer.test(model, datamodule=datamodule_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5fe8b",
   "metadata": {},
   "source": [
    "### Start Training with dataset B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce213d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "families_B = [\n",
    "    \"CurveFault_B\",\n",
    "    \"CurveVel_B\",\n",
    "    \"FlatFault_B\",\n",
    "    \"FlatVel_B\",\n",
    "    \"Style_B\",\n",
    "]\n",
    "\n",
    "train_paths_B = train_paths.query(\"family in @families_B\")\n",
    "display(train_paths_B)\n",
    "display(pd.crosstab(train_paths_B[\"family\"], train_paths_B[\"label\"]))\n",
    "display(pd.crosstab(valid_paths[\"family\"], valid_paths[\"label\"]))\n",
    "display(pd.crosstab(test_paths[\"family\"], test_paths[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "model.learning_rate = 1e-04\n",
    "\n",
    "datamodule_B = MyDataModule(\n",
    "    train_paths=train_paths_B,\n",
    "    valid_paths=valid_paths,\n",
    "    test_paths=test_paths,\n",
    "    seed=config.seed,\n",
    "    batch_size=config.batch_size,\n",
    "    mean_x=statistics_A[\"All\"][\"mean_x\"],\n",
    "    std_x=statistics_A[\"All\"][\"std_x\"],\n",
    "    mean_y=None,\n",
    "    std_y=None,\n",
    ")\n",
    "\n",
    "callbacks=[\n",
    "    EarlyStopping(monitor=\"val_f1\", patience=config.patience, mode='max'),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "    TQDMProgressBar(),\n",
    "    StochasticWeightAveraging(\n",
    "        swa_lrs=1e-6,\n",
    "        swa_epoch_start=int(0.8*config.epochs),\n",
    "        annealing_epochs=int(0.2*config.epochs),\n",
    "    ),\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=config.output_dir,\n",
    "    enable_checkpointing=False,\n",
    "    accelerator=\"cuda\",\n",
    "    max_epochs=config.epochs,\n",
    "    precision=\"bf16-mixed\",\n",
    "    callbacks=callbacks,\n",
    "    logger=CSVLogger(config.output_dir, name=\"classifier_B\"),\n",
    "    log_every_n_steps=1,\n",
    "    val_check_interval=None,\n",
    "    check_val_every_n_epoch=1,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule_B)\n",
    "trainer.test(model, datamodule=datamodule_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2583a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics0 = pd.read_csv(config.output_dir.joinpath(\"classifier_A/version_0/metrics.csv\"))\n",
    "metrics0 = metrics0.sort_values([\"step\", \"epoch\"]).reset_index(drop=True)\n",
    "display(metrics0.head())\n",
    "display(metrics0[[\"epoch\", \"val_f1_epoch\"]].dropna())\n",
    "\n",
    "_, axs = plt.subplots(3, 1)\n",
    "metrics0[[\"step\", \"lr-AdamW\"]].dropna().plot(x=\"step\", y=\"lr-AdamW\", kind=\"line\", marker=\".\", ax=axs[0])\n",
    "metrics0[[\"epoch\", \"val_f1_epoch\"]].dropna().plot(x=\"epoch\", y=\"val_f1_epoch\", kind=\"line\", marker=\".\", ax=axs[1])\n",
    "metrics0[[\"epoch\", \"val_loss_epoch\"]].dropna().plot(x=\"epoch\", y=\"val_loss_epoch\", kind=\"line\", marker=\".\", ax=axs[2])\n",
    "axs[0].set_xlabel(\"step\")\n",
    "axs[0].set_ylabel(\"learning rate\")\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].set_ylabel(\"F1\")\n",
    "axs[2].set_xlabel(\"epoch\")\n",
    "axs[2].set_ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics1 = pd.read_csv(config.output_dir.joinpath(\"classifier_B/version_0/metrics.csv\"))\n",
    "metrics1 = metrics1.sort_values([\"step\", \"epoch\"]).reset_index(drop=True)\n",
    "display(metrics1.head())\n",
    "display(metrics1[[\"epoch\", \"val_f1_epoch\"]].dropna())\n",
    "\n",
    "_, axs = plt.subplots(3, 1)\n",
    "metrics1[[\"step\", \"lr-AdamW\"]].dropna().plot(x=\"step\", y=\"lr-AdamW\", kind=\"line\", marker=\".\", ax=axs[0])\n",
    "metrics1[[\"epoch\", \"val_f1_epoch\"]].dropna().plot(x=\"epoch\", y=\"val_f1_epoch\", kind=\"line\", marker=\".\", ax=axs[1])\n",
    "metrics1[[\"epoch\", \"val_loss_epoch\"]].dropna().plot(x=\"epoch\", y=\"val_loss_epoch\", kind=\"line\", marker=\".\", ax=axs[2])\n",
    "axs[0].set_xlabel(\"step\")\n",
    "axs[0].set_ylabel(\"learning rate\")\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].set_ylabel(\"F1\")\n",
    "axs[2].set_xlabel(\"epoch\")\n",
    "axs[2].set_ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afce58",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = config.output_dir.joinpath(f\"classifier_{config.seed}.ckpt\")\n",
    "trainer.save_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7dc7ab",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            paths: List[Path],\n",
    "            mean_x: Tuple[float],\n",
    "            std_x: Tuple[float],\n",
    "        ) -> None:\n",
    "        self.paths = paths\n",
    "        self.transform_x = A.Compose([\n",
    "            A.Normalize(mean=mean_x, std=std_x),\n",
    "            A.Resize(512, 72),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        path = self.paths[index]\n",
    "        images = np.load(path)\n",
    "        x = images[\"x\"].transpose(1, 2, 0)\n",
    "        x = self.transform_x(image=x)[\"image\"]\n",
    "        x = torch.from_numpy(x).permute(2, 0, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbac890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitClassifierModel.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model.device)\n",
    "\n",
    "test_dataset = TestDataset(\n",
    "    paths=list(test_paths[\"path\"]),\n",
    "    mean_x=statistics_A[\"All\"][\"mean_x\"],\n",
    "    std_x=statistics_A[\"All\"][\"std_x\"],\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=os.cpu_count()//2,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ea337",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    default_root_dir=config.output_dir,\n",
    "    enable_checkpointing=False,\n",
    ")\n",
    "test_logits = trainer.predict(model, test_dataloader)\n",
    "test_logits = torch.cat(test_logits)\n",
    "print(test_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4749fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
