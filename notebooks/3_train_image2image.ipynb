{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, StochasticWeightAveraging, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchmetrics.functional.classification import multiclass_confusion_matrix, multiclass_f1_score\n",
    "\n",
    "from configs.config import CFG\n",
    "from model.image2image_unet import Image2ImageUNet\n",
    "from util.my_dataset import MyDataModule, MyDataset\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38572a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm import list_models\n",
    "\n",
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ee4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitUNetModel(L.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            learning_rate: float,\n",
    "            mean_y: float,\n",
    "            std_y: float,\n",
    "        ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.model = Image2ImageUNet(model_name)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mean_y = mean_y\n",
    "        self.std_y = std_y\n",
    "        self.save_hyperparameters(ignore=['criterion'])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        logit = self.model(x)\n",
    "        return logit\n",
    "\n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
    "        x, y, _ = batch\n",
    "        logit = self.forward(x)\n",
    "        loss = self.criterion(logit, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
    "        x, y, _ = batch\n",
    "        logit = self.forward(x)\n",
    "        loss = self.criterion(logit, y)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(\n",
    "            self,\n",
    "            batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "            batch_idx: int\n",
    "        ) -> Dict[str, float]:\n",
    "        \n",
    "        x, y, _ = batch\n",
    "        logit = self.forward(x)\n",
    "        logit = logit * self.std_y + self.mean_y\n",
    "        logit = torch.clip(logit, min=1500, max=4500)\n",
    "        y = y * self.std_y + self.mean_y\n",
    "        loss = F.l1_loss(logit, y)\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            _, axs = plt.subplots(1, 8, figsize=(32, 8))\n",
    "            for i in range(5):\n",
    "                im0 = axs[i].imshow(x.float()[0, i].cpu(), aspect=\"auto\")\n",
    "                plt.colorbar(im0, ax=axs[i])\n",
    "            im1 = axs[5].imshow(logit.float()[0, 0].cpu(), aspect=\"auto\")\n",
    "            im2 = axs[6].imshow(y.float()[0, 0].cpu(), aspect=\"auto\")\n",
    "            im3 = axs[7].imshow(y.float()[0, 0].cpu()-logit.float()[0, 0].cpu(), aspect=\"auto\")\n",
    "            plt.colorbar(im1, ax=axs[5])\n",
    "            plt.colorbar(im2, ax=axs[6])\n",
    "            plt.colorbar(im3, ax=axs[7])\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def predict_step(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logit = self.forward(x)\n",
    "        logit = logit * self.std_y + self.mean_y\n",
    "        return logit\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, object]:\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optimizer,\n",
    "            max_lr=self.learning_rate,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=0.3,\n",
    "            div_factor=25,\n",
    "            final_div_factor=1e+04,\n",
    "        )\n",
    "        scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"strict\": False,\n",
    "        }\n",
    "        return (\n",
    "            {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": scheduler_config,\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329b68f",
   "metadata": {},
   "source": [
    "### Define Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceeb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_time = datetime.datetime.now()\n",
    "output_dir = Path(f\"../output/image2image_{now_time.date()}-{now_time.hour:02}-{now_time.minute:02}\")\n",
    "\n",
    "config = CFG(\n",
    "    output_dir=output_dir,\n",
    "    model_name=\"convnextv2_tiny\",\n",
    "    debag=False,\n",
    "    train_ratio=0.8,\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    patience=5,\n",
    ")\n",
    "config.seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406304b",
   "metadata": {},
   "source": [
    "### Load Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4072e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = Path(\"../data\")\n",
    "print([p.stem for p in dir_path.glob(\"*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "families = {\n",
    "    \"CurveFault_A\": 0,\n",
    "    \"CurveFault_B\": 0,\n",
    "    \"CurveVel_A\": 1,\n",
    "    \"CurveVel_B\": 1,\n",
    "    \"FlatFault_A\": 2,\n",
    "    \"FlatFault_B\": 2,\n",
    "    \"FlatVel_A\": 3,\n",
    "    \"FlatVel_B\": 3,\n",
    "    \"Style_A\": 4,\n",
    "    \"Style_B\": 4, \n",
    "}\n",
    "\n",
    "paths = []\n",
    "for family, label in families.items():\n",
    "    for i, p in enumerate(dir_path.joinpath(family).glob(\"*.npz\")):\n",
    "        paths.append((family, label, p))\n",
    "paths = pd.DataFrame(paths, columns=[\"family\", \"label\", \"path\"])\n",
    "if config.debag:\n",
    "    paths = paths.sample(n=10_000, replace=False)\n",
    "display(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4301bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_parent(x: Path):\n",
    "#     parent = \"_\".join(x.stem.split(\"_\")[:-1])\n",
    "#     return parent\n",
    "\n",
    "\n",
    "# paths[\"path\"] = \"../data/\" + paths[\"family\"] + \"/\" + paths[\"path\"].apply(get_parent)\n",
    "# paths = paths[[\"family\", \"label\", \"path\"]].drop_duplicates()\n",
    "# paths = paths.sort_values([\"family\", \"path\"]).reset_index(drop=True)\n",
    "# if config.debag:\n",
    "#     paths = paths.sample(n=100, replace=False).reset_index(drop=True)\n",
    "# display(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9a2fc",
   "metadata": {},
   "source": [
    "### Split Paths into training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f71f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_paths, test_paths = train_test_split(\n",
    "    paths,\n",
    "    train_size=config.train_ratio,\n",
    "    shuffle=True,\n",
    "    random_state=config.seed,\n",
    "    stratify=paths[\"family\"]\n",
    ")\n",
    "train_paths, valid_paths = train_test_split(\n",
    "    train_valid_paths,\n",
    "    train_size=config.train_ratio,\n",
    "    shuffle=True,\n",
    "    random_state=config.seed,\n",
    "    stratify=train_valid_paths[\"family\"]\n",
    ")\n",
    "display(train_paths)\n",
    "display(valid_paths)\n",
    "display(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/statistics.pkl\", \"rb\") as f:\n",
    "    statistics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd68131",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_pairs = {\n",
    "    \"CurveFault\": (\"CurveFault_A\", \"CurveFault_B\"),\n",
    "    # \"CurveVel\": (\"CurveVel_A\", \"CurveVel_B\"),\n",
    "    # \"FlatFault\": (\"FlatFault_A\", \"FlatFault_B\"),\n",
    "    # \"FlatVel\": (\"FlatVel_A\", \"FlatVel_B\"),\n",
    "    # \"Style\": (\"Style_A\", \"Style_B\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53820bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "for family, (family_A, family_B) in family_pairs.items():\n",
    "    print(family_A, family_B)\n",
    "    print(statistics[family_A][\"mean_y\"], statistics[family_A][\"std_y\"])\n",
    "\n",
    "    model = LitUNetModel(\n",
    "        model_name=config.model_name,\n",
    "        learning_rate=1e-03,\n",
    "        mean_y=statistics[family_A][\"mean_y\"],\n",
    "        std_y=statistics[family_A][\"std_y\"],\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    train with dataset A\n",
    "    \"\"\"\n",
    "    train_paths_A = train_paths.query(\"family == @family_A\")\n",
    "    valid_paths_A = valid_paths.query(\"family == @family_A\")\n",
    "    test_paths_A = test_paths.query(\"family == @family_A\")\n",
    "    display(pd.crosstab(train_paths_A[\"family\"], train_paths_A[\"label\"]))\n",
    "    display(pd.crosstab(valid_paths_A[\"family\"], valid_paths_A[\"label\"]))\n",
    "    display(pd.crosstab(test_paths_A[\"family\"], test_paths_A[\"label\"]))\n",
    "\n",
    "    datamodule_A = MyDataModule(\n",
    "        train_paths=train_paths_A,\n",
    "        valid_paths=valid_paths_A,\n",
    "        test_paths=test_paths_A,\n",
    "        seed=config.seed,\n",
    "        batch_size=config.batch_size,\n",
    "        mean_x=statistics[family_A][\"mean_log_x\"],\n",
    "        std_x=statistics[family_A][\"std_log_x\"],\n",
    "        mean_y=statistics[family_A][\"mean_y\"],\n",
    "        std_y=statistics[family_A][\"mean_y\"],\n",
    "    )\n",
    "    del train_paths_A, valid_paths_A, test_paths_A\n",
    "\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=config.patience, mode='min'),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        TQDMProgressBar(),\n",
    "        # StochasticWeightAveraging(\n",
    "        #     swa_lrs=1e-5,\n",
    "        #     swa_epoch_start=int(0.8*config.epochs),\n",
    "        #     annealing_epochs=int(0.2*config.epochs),\n",
    "        # ),\n",
    "    ]\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=config.output_dir,\n",
    "        enable_checkpointing=False,\n",
    "        accelerator=\"cuda\",\n",
    "        max_epochs=config.epochs,\n",
    "        precision=\"bf16-mixed\",\n",
    "        callbacks=callbacks,\n",
    "        logger=CSVLogger(config.output_dir, name=family),\n",
    "        log_every_n_steps=150,\n",
    "        val_check_interval=None,\n",
    "        check_val_every_n_epoch=1,\n",
    "        accumulate_grad_batches=1,\n",
    "        gradient_clip_val=0,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=datamodule_A)\n",
    "    trainer.test(model, datamodule=datamodule_A)\n",
    "    del datamodule_A\n",
    "    del callbacks\n",
    "    del trainer\n",
    "\n",
    "    \"\"\"\n",
    "    train with dataset B\n",
    "    \"\"\"\n",
    "    model.learning_rate = 1e-03\n",
    "\n",
    "    train_paths_B = train_paths.query(\"family == @family_B\")\n",
    "    valid_paths_B = valid_paths.query(\"family == @family_B\")\n",
    "    test_paths_B = test_paths.query(\"family == @family_B\")\n",
    "    display(pd.crosstab(train_paths_B[\"family\"], train_paths_B[\"label\"]))\n",
    "    display(pd.crosstab(valid_paths_B[\"family\"], valid_paths_B[\"label\"]))\n",
    "    display(pd.crosstab(test_paths_B[\"family\"], test_paths_B[\"label\"]))\n",
    "\n",
    "    datamodule_B = MyDataModule(\n",
    "        train_paths=train_paths_B,\n",
    "        valid_paths=valid_paths_B,\n",
    "        test_paths=test_paths_B,\n",
    "        seed=config.seed,\n",
    "        batch_size=config.batch_size,\n",
    "        mean_x=statistics[family_A][\"mean_log_x\"],\n",
    "        std_x=statistics[family_A][\"std_log_x\"],\n",
    "        mean_y=statistics[family_A][\"mean_y\"],\n",
    "        std_y=statistics[family_A][\"std_y\"],\n",
    "    )\n",
    "    del train_paths_B, valid_paths_B, test_paths_B\n",
    "\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=config.patience, mode='min'),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        TQDMProgressBar(),\n",
    "        # StochasticWeightAveraging(\n",
    "        #     swa_lrs=1e-6,\n",
    "        #     swa_epoch_start=int(0.8*config.epochs),\n",
    "        #     annealing_epochs=int(0.2*config.epochs),\n",
    "        # ),\n",
    "    ]\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=config.output_dir,\n",
    "        enable_checkpointing=False,\n",
    "        accelerator=\"cuda\",\n",
    "        max_epochs=config.epochs,\n",
    "        precision=\"bf16-mixed\",\n",
    "        callbacks=callbacks,\n",
    "        logger=CSVLogger(config.output_dir, name=family),\n",
    "        log_every_n_steps=150,\n",
    "        val_check_interval=None,\n",
    "        check_val_every_n_epoch=1,\n",
    "        accumulate_grad_batches=1,\n",
    "        gradient_clip_val=0,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=datamodule_B)\n",
    "    trainer.test(model, datamodule=datamodule_B)\n",
    "\n",
    "    checkpoint_path = config.output_dir.joinpath(f\"{family}/image2image_{family}_{config.seed}.ckpt\")\n",
    "    trainer.save_checkpoint(checkpoint_path)\n",
    "    del datamodule_B\n",
    "    del callbacks\n",
    "    del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "for family, (family_A, family_B) in family_pairs.items():\n",
    "    print(family_A, family_B)\n",
    "    print(statistics[family_A][\"mean_y\"], statistics[family_A][\"std_y\"])\n",
    "\n",
    "    model = LitUNetModel(\n",
    "        model_name=config.model_name,\n",
    "        learning_rate=1e-03,\n",
    "        mean_y=statistics[family_A][\"mean_y\"],\n",
    "        std_y=statistics[family_A][\"std_y\"],\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    train with dataset A\n",
    "    \"\"\"\n",
    "    train_paths_A = train_paths.query(\"family == @family_A\")\n",
    "    valid_paths_A = valid_paths.query(\"family == @family_A\")\n",
    "    test_paths_A = test_paths.query(\"family == @family_A\")\n",
    "    display(pd.crosstab(train_paths_A[\"family\"], train_paths_A[\"label\"]))\n",
    "    display(pd.crosstab(valid_paths_A[\"family\"], valid_paths_A[\"label\"]))\n",
    "    display(pd.crosstab(test_paths_A[\"family\"], test_paths_A[\"label\"]))\n",
    "\n",
    "    datamodule_A = MyDataModule(\n",
    "        train_paths=train_paths_A,\n",
    "        valid_paths=valid_paths_A,\n",
    "        test_paths=test_paths_A,\n",
    "        seed=config.seed,\n",
    "        batch_size=config.batch_size,\n",
    "        mean_x=statistics[family_A][\"mean_log_x\"],\n",
    "        std_x=statistics[family_A][\"std_log_x\"],\n",
    "        mean_y=statistics[family_A][\"mean_y\"],\n",
    "        std_y=statistics[family_A][\"mean_y\"],\n",
    "    )\n",
    "    del train_paths_A, valid_paths_A, test_paths_A\n",
    "\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=config.patience, mode='min'),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        TQDMProgressBar(),\n",
    "        # StochasticWeightAveraging(\n",
    "        #     swa_lrs=1e-5,\n",
    "        #     swa_epoch_start=int(0.8*config.epochs),\n",
    "        #     annealing_epochs=int(0.2*config.epochs),\n",
    "        # ),\n",
    "    ]\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=config.output_dir,\n",
    "        enable_checkpointing=False,\n",
    "        accelerator=\"cuda\",\n",
    "        max_epochs=config.epochs,\n",
    "        precision=\"bf16-mixed\",\n",
    "        callbacks=callbacks,\n",
    "        logger=CSVLogger(config.output_dir, name=family),\n",
    "        log_every_n_steps=150,\n",
    "        val_check_interval=None,\n",
    "        check_val_every_n_epoch=1,\n",
    "        accumulate_grad_batches=1,\n",
    "        gradient_clip_val=0,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=datamodule_A)\n",
    "    trainer.test(model, datamodule=datamodule_A)\n",
    "    del datamodule_A\n",
    "    del callbacks\n",
    "    del trainer\n",
    "\n",
    "    \"\"\"\n",
    "    train with dataset B\n",
    "    \"\"\"\n",
    "    model.learning_rate = 1e-04\n",
    "\n",
    "    train_paths_B = train_paths.query(\"family == @family_B\")\n",
    "    valid_paths_B = valid_paths.query(\"family == @family_B\")\n",
    "    test_paths_B = test_paths.query(\"family == @family_B\")\n",
    "    display(pd.crosstab(train_paths_B[\"family\"], train_paths_B[\"label\"]))\n",
    "    display(pd.crosstab(valid_paths_B[\"family\"], valid_paths_B[\"label\"]))\n",
    "    display(pd.crosstab(test_paths_B[\"family\"], test_paths_B[\"label\"]))\n",
    "\n",
    "    datamodule_B = MyDataModule(\n",
    "        train_paths=train_paths_B,\n",
    "        valid_paths=valid_paths_B,\n",
    "        test_paths=test_paths_B,\n",
    "        seed=config.seed,\n",
    "        batch_size=config.batch_size,\n",
    "        mean_x=statistics[family_A][\"mean_log_x\"],\n",
    "        std_x=statistics[family_A][\"std_log_x\"],\n",
    "        mean_y=statistics[family_A][\"mean_y\"],\n",
    "        std_y=statistics[family_A][\"std_y\"],\n",
    "    )\n",
    "    del train_paths_B, valid_paths_B, test_paths_B\n",
    "\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=config.patience, mode='min'),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        TQDMProgressBar(),\n",
    "        # StochasticWeightAveraging(\n",
    "        #     swa_lrs=1e-6,\n",
    "        #     swa_epoch_start=int(0.8*config.epochs),\n",
    "        #     annealing_epochs=int(0.2*config.epochs),\n",
    "        # ),\n",
    "    ]\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=config.output_dir,\n",
    "        enable_checkpointing=False,\n",
    "        accelerator=\"cuda\",\n",
    "        max_epochs=config.epochs,\n",
    "        precision=\"bf16-mixed\",\n",
    "        callbacks=callbacks,\n",
    "        logger=CSVLogger(config.output_dir, name=family),\n",
    "        log_every_n_steps=150,\n",
    "        val_check_interval=None,\n",
    "        check_val_every_n_epoch=1,\n",
    "        accumulate_grad_batches=1,\n",
    "        gradient_clip_val=0,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=datamodule_B)\n",
    "    trainer.test(model, datamodule=datamodule_B)\n",
    "\n",
    "    checkpoint_path = config.output_dir.joinpath(f\"{family}/image2image_{family}_{config.seed}.ckpt\")\n",
    "    trainer.save_checkpoint(checkpoint_path)\n",
    "    del datamodule_B\n",
    "    del callbacks\n",
    "    del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2583a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for family, (family_A, family_B) in family_pairs.items():\n",
    "    metrics0 = pd.read_csv(config.output_dir.joinpath(f\"{family}/version_0/metrics.csv\"))\n",
    "    metrics0 = metrics0.sort_values([\"step\", \"epoch\"]).reset_index(drop=True)\n",
    "    display(metrics0.head())\n",
    "    display(metrics0[[\"epoch\", \"val_loss_epoch\"]].dropna())\n",
    "\n",
    "    _, axs = plt.subplots(2, 1)\n",
    "    metrics0[[\"step\", \"lr-AdamW\"]].dropna().plot(x=\"step\", y=\"lr-AdamW\", kind=\"line\", marker=\".\", ax=axs[0])\n",
    "    metrics0[[\"epoch\", \"val_loss_epoch\"]].dropna().plot(x=\"epoch\", y=\"val_loss_epoch\", kind=\"line\", marker=\".\", ax=axs[1])\n",
    "    axs[0].set_xlabel(\"step\")\n",
    "    axs[0].set_ylabel(\"learning rate\")\n",
    "    axs[1].set_xlabel(\"epoch\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    metrics1 = pd.read_csv(config.output_dir.joinpath(f\"{family}/version_1/metrics.csv\"))\n",
    "    metrics1 = metrics1.sort_values([\"step\", \"epoch\"]).reset_index(drop=True)\n",
    "    display(metrics1.head())\n",
    "    display(metrics1[[\"epoch\", \"val_loss_epoch\"]].dropna())\n",
    "\n",
    "    _, axs = plt.subplots(2, 1)\n",
    "    metrics1[[\"step\", \"lr-AdamW\"]].dropna().plot(x=\"step\", y=\"lr-AdamW\", kind=\"line\", marker=\".\", ax=axs[0])\n",
    "    metrics1[[\"epoch\", \"val_loss_epoch\"]].dropna().plot(x=\"epoch\", y=\"val_loss_epoch\", kind=\"line\", marker=\".\", ax=axs[1])\n",
    "    axs[0].set_xlabel(\"step\")\n",
    "    axs[0].set_ylabel(\"learning rate\")\n",
    "    axs[1].set_xlabel(\"epoch\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7dc7ab",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.log_transform import log_transform_torch\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            paths: List[Path],\n",
    "            mean_x: Union[np.ndarray, Tuple[float]],\n",
    "            std_x: Union[np.ndarray, Tuple[float]],\n",
    "        ) -> None:\n",
    "        \n",
    "        self.paths = paths\n",
    "        self.mean_x = torch.tensor(mean_x).unsqueeze(dim=1).unsqueeze(dim=2)\n",
    "        self.std_x = torch.tensor(std_x).unsqueeze(dim=1).unsqueeze(dim=2)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        path = self.paths[index]\n",
    "        images = np.load(path)\n",
    "        x = images[\"x\"] # (5, 1000, 70)\n",
    "        x = torch.from_numpy(x)\n",
    "        x = log_transform_torch(x)\n",
    "        x = (x - self.mean_x) / self.std_x\n",
    "        x = x.unsqueeze(dim=0) # (1, 5, 1000, 70)\n",
    "        x = F.interpolate(x, size=(288, 288), mode=\"nearest\")\n",
    "        x = x.squeeze(dim=0)\n",
    "        x = x.float()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc564a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.crosstab(test_paths[\"family\"], test_paths[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbac890",
   "metadata": {},
   "outputs": [],
   "source": [
    "for family, (family_A, family_B) in family_pairs.items():\n",
    "    print(family)\n",
    "    checkpoint_path = config.output_dir.joinpath(f\"{family}/image2image_{family}_{config.seed}.ckpt\")\n",
    "\n",
    "    model = LitUNetModel.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(model.device)\n",
    "\n",
    "    test_dataset = TestDataset(\n",
    "        paths=list(test_paths.query(\"family == @family_A or family == @family_B\")[\"path\"]),\n",
    "        mean_x=statistics[family_A][\"mean_log_x\"],\n",
    "        std_x=statistics[family_A][\"std_log_x\"],\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=os.cpu_count()//8,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=config.output_dir,\n",
    "        enable_checkpointing=False,\n",
    "    )\n",
    "    test_logits = trainer.predict(model, test_dataloader)\n",
    "    test_logits = torch.cat(test_logits)\n",
    "    print(test_logits.shape, test_logits.min(), test_logits.max())\n",
    "\n",
    "    random_index = np.random.choice(range(len(test_dataset)), size=5, replace=False)\n",
    "    print(random_index)\n",
    "\n",
    "    _, axs = plt.subplots(1, 5, figsize=(12, 4))\n",
    "    for e, i in enumerate(random_index):\n",
    "        img = axs[e].imshow(test_logits[i, 0], aspect=\"auto\")\n",
    "        plt.colorbar(img, ax=axs[e])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4749fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
