{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint, StochasticWeightAveraging, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchmetrics.functional.classification import multiclass_confusion_matrix, multiclass_f1_score\n",
    "\n",
    "from configs.config import CFG\n",
    "from ema.ema import EMACallback\n",
    "from model.multitask_unet import MultiTaskUNet\n",
    "from util.get_logger import get_logger\n",
    "from util.my_dataset import MyDataModule, MyDataset\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm import list_models\n",
    "\n",
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ee4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm.optim\n",
    "\n",
    "\n",
    "class LitUNetModel(L.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            pretrained: bool,\n",
    "            num_classes: int,\n",
    "            height: int,\n",
    "            width: int,\n",
    "            learning_rate: float,\n",
    "            mean_y: torch.Tensor,\n",
    "            std_y: torch.Tensor,\n",
    "        ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.model = MultiTaskUNet(model_name, num_classes, pretrained, height, width)\n",
    "        self.num_classes = num_classes\n",
    "        self.criterion1 = nn.L1Loss()\n",
    "        self.criterion2 = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mean_y = mean_y\n",
    "        self.std_y = std_y\n",
    "        self.alpha = 1e-02\n",
    "        self.save_hyperparameters(ignore=[\"criterion1\", \"criterion2\"])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        reg_logit, clf_logit = self.model(x)\n",
    "        pred_class = clf_logit.argmax(dim=1)\n",
    "        mean_by_class = self.mean_y[pred_class]\n",
    "        std_by_class = self.std_y[pred_class]\n",
    "        reg_logit = reg_logit * std_by_class + mean_by_class\n",
    "        reg_logit = torch.clip(reg_logit, min=1500, max=4500)\n",
    "        return reg_logit, clf_logit\n",
    "\n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
    "        x, y, label, _ = batch\n",
    "        batch_size = len(x)\n",
    "        reg_logit, clf_logit = self.forward(x)\n",
    "        loss1 = self.criterion1(reg_logit, y)\n",
    "        loss2 = self.criterion2(clf_logit, label)\n",
    "        loss3 = self._edge_loss(reg_logit, y)\n",
    "        loss = loss1 + loss2 + self.alpha * loss3\n",
    "        lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "        self.log(\"train_loss1\", loss1, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"train_loss2\", loss2, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"train_loss3\", loss3, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"lr\", lr, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
    "        x, y, label, _ = batch\n",
    "        batch_size = len(x)\n",
    "        reg_logit, clf_logit = self.forward(x)\n",
    "        loss1 = F.l1_loss(reg_logit, y)\n",
    "        loss2 = F.cross_entropy(clf_logit, label)\n",
    "        loss3 = self._edge_loss(reg_logit, y)\n",
    "        loss = loss1 + loss2 + self.alpha * loss3\n",
    "        f1 = multiclass_f1_score(clf_logit, label, num_classes=self.num_classes, average=\"macro\")\n",
    "        self.log(\"val_loss1\", loss1, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"val_loss2\", loss2, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"val_loss3\", loss3, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"val_f1\", f1, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        return loss\n",
    "    \n",
    "    def on_test_epoch_start(self):\n",
    "        self.clf_targets = []\n",
    "        self.clf_preds = []\n",
    "        self.mae_all = 0\n",
    "        self.num_data = 0\n",
    "    \n",
    "    def test_step(\n",
    "            self,\n",
    "            batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "            batch_idx: int\n",
    "        ) -> Dict[str, float]:\n",
    "        \n",
    "        x, y, label, path = batch\n",
    "        batch_size = len(x)\n",
    "        reg_logit, clf_logit = self.forward(x)\n",
    "        loss1 = F.l1_loss(reg_logit, y)\n",
    "        loss2 = F.cross_entropy(clf_logit, label)\n",
    "        loss3 = self._edge_loss(reg_logit, y)\n",
    "        loss = loss1 + loss2 + self.alpha * loss3\n",
    "        f1 = multiclass_f1_score(clf_logit, label, num_classes=self.num_classes, average=\"macro\")\n",
    "        self.mae_all += F.l1_loss(reg_logit, y, reduction=\"sum\")\n",
    "        self.num_data += len(x)\n",
    "\n",
    "        self.clf_targets.append(label.cpu())\n",
    "        self.clf_preds.append(clf_logit.argmax(dim=1).cpu())\n",
    "        \n",
    "        self.log(\"test_loss1\", loss1, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"test_loss2\", loss2, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"test_loss3\", loss3, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"test_f1\", f1, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        \n",
    "        # if batch_idx == 0:\n",
    "        #     print(path[0])\n",
    "        #     _, axs = plt.subplots(1, 8, figsize=(32, 8))\n",
    "        #     for i in range(5):\n",
    "        #         im0 = axs[i].imshow(x.float()[0, i].cpu(), aspect=\"auto\")\n",
    "        #         plt.colorbar(im0, ax=axs[i])\n",
    "        #     im1 = axs[5].imshow(reg_logit.float()[0, 0].cpu(), aspect=\"auto\")\n",
    "        #     im2 = axs[6].imshow(y.float()[0, 0].cpu(), aspect=\"auto\")\n",
    "        #     im3 = axs[7].imshow(y.float()[0, 0].cpu()-reg_logit.float()[0, 0].cpu(), aspect=\"auto\")\n",
    "        #     plt.colorbar(im1, ax=axs[5])\n",
    "        #     plt.colorbar(im2, ax=axs[6])\n",
    "        #     plt.colorbar(im3, ax=axs[7])\n",
    "        #     plt.suptitle(path[0])\n",
    "        #     plt.tight_layout()\n",
    "        #     plt.show()\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        clf_targets = torch.cat(self.clf_targets)\n",
    "        clf_preds = torch.cat(self.clf_preds)\n",
    "        cm = multiclass_confusion_matrix(clf_preds, clf_targets, num_classes=self.num_classes)\n",
    "        print(cm)\n",
    "        print(f\"Test MAE: {self.mae_all / self.num_data / 70 / 70:.4f}\")\n",
    "        print(f\"# of test data: {self.num_data}\")\n",
    "        del self.clf_targets\n",
    "        del self.clf_preds\n",
    "    \n",
    "    def predict_step(self, batch: List[Union[Tuple[str], torch.Tensor]]) -> Tuple[str, torch.Tensor, torch.Tensor]:\n",
    "        file_names, x = batch\n",
    "        reg_logit, clf_logit = self.forward(x)\n",
    "        return file_names, reg_logit, clf_logit\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, object]:\n",
    "        self.optimizer = timm.optim.create_optimizer_v2(self, opt=\"adamw\", lr=self.learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=self.optimizer,\n",
    "            max_lr=self.learning_rate,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=0.1,\n",
    "            div_factor=25,\n",
    "            final_div_factor=1e+04,\n",
    "        )\n",
    "        scheduler_config = {\n",
    "            \"scheduler\": self.scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"strict\": False,\n",
    "        }\n",
    "        return (\n",
    "            {\n",
    "                \"optimizer\": self.optimizer,\n",
    "                \"lr_scheduler\": scheduler_config,\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    def _total_variation_loss(self, img):\n",
    "        loss = (\n",
    "            torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]))\n",
    "            + torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]))\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def _get_sobel_edges(self, x):\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=x.dtype, device=x.device).view(1, 1, 3, 3)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=x.dtype, device=x.device).view(1, 1, 3, 3)\n",
    "        \n",
    "        if x.shape[1] > 1:\n",
    "            x = x.mean(dim=1, keepdim=True)\n",
    "\n",
    "        grad_x = F.conv2d(x, sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(x, sobel_y, padding=1)\n",
    "        edge = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-6)\n",
    "        return edge\n",
    "    \n",
    "    def _edge_loss(self, pred, target):\n",
    "        pred_edge = self._get_sobel_edges(pred)\n",
    "        target_edge = self._get_sobel_edges(target)\n",
    "        return F.l1_loss(pred_edge, target_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329b68f",
   "metadata": {},
   "source": [
    "### Define Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceeb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_time = datetime.datetime.now()\n",
    "output_dir = Path(f\"../output/multitask_using_AB_v2_{now_time.date()}-{now_time.hour:02}-{now_time.minute:02}\")\n",
    "\n",
    "config = CFG(\n",
    "    output_dir=output_dir,\n",
    "    # model_name=\"convnextv2_base\",\n",
    "    model_name=\"focalnet_base_lrf\",\n",
    "    pretrained=True,\n",
    "    debag=False,\n",
    "    train_ratio=0.8,\n",
    "    seed=42,\n",
    "    height=288,\n",
    "    width=288,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    learning_rate=4e-04,\n",
    "    patience=10,\n",
    "    accumulation_steps=4,\n",
    "    do_transform=True,\n",
    ")\n",
    "config.seed_everything()\n",
    "\n",
    "logger = get_logger(output_dir.joinpath('output.log'))\n",
    "config_log = [\n",
    "    f'{k} = {config.__dict__[k]}'\n",
    "    for k, _ in config.__dict__.items()\n",
    "    if not k.startswith('__')\n",
    "]\n",
    "logger.info('\\n'.join(config_log))\n",
    "logger.info('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406304b",
   "metadata": {},
   "source": [
    "### Load Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4072e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = Path(\"../data\")\n",
    "print([p.stem for p in dir_path.glob(\"*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "families = {\n",
    "    \"CurveFault_A\": 0,\n",
    "    \"CurveFault_B\": 1,\n",
    "    \"CurveVel_A\": 2,\n",
    "    \"CurveVel_B\": 3,\n",
    "    \"FlatFault_A\": 4,\n",
    "    \"FlatFault_B\": 5,\n",
    "    \"FlatVel_A\": 6,\n",
    "    \"FlatVel_B\": 7,\n",
    "    \"Style_A\": 8,\n",
    "    \"Style_B\": 9,\n",
    "}\n",
    "\n",
    "paths = []\n",
    "for family, label in families.items():\n",
    "    for i, p in enumerate(dir_path.joinpath(family).glob(\"*.npz\")):\n",
    "        paths.append((family, label, p))\n",
    "paths = pd.DataFrame(paths, columns=[\"family\", \"label\", \"path\"])\n",
    "if config.debag:\n",
    "    paths = paths.sample(n=5_000, replace=False)\n",
    "display(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9a2fc",
   "metadata": {},
   "source": [
    "### Split Paths into training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f71f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_paths, test_paths = train_test_split(\n",
    "    paths,\n",
    "    train_size=config.train_ratio,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=paths[\"family\"]\n",
    ")\n",
    "train_paths, valid_paths = train_test_split(\n",
    "    train_valid_paths,\n",
    "    train_size=config.train_ratio,\n",
    "    shuffle=True,\n",
    "    random_state=config.seed,\n",
    "    stratify=train_valid_paths[\"family\"]\n",
    ")\n",
    "display(train_paths)\n",
    "display(valid_paths)\n",
    "display(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = []\n",
    "std_x = []\n",
    "mean_y = []\n",
    "std_y = []\n",
    "with open(\"../output/statistics.pkl\", \"rb\") as f:\n",
    "    statistics = pickle.load(f)\n",
    "    mean_x.append(list(statistics[\"All\"][\"mean_log_x\"]))\n",
    "    std_x.append(list(statistics[\"All\"][\"std_log_x\"]))\n",
    "    for f in families.keys():\n",
    "        mean_y.append(statistics[f][\"mean_y\"])\n",
    "        std_y.append(statistics[f][\"std_y\"])\n",
    "mean_x = torch.tensor(mean_x).reshape(-1, 1, 1)\n",
    "std_x = torch.tensor(std_x).reshape(-1, 1, 1)\n",
    "mean_y = torch.tensor(mean_y).reshape(-1, 1, 1, 1)\n",
    "std_y = torch.tensor(std_y).reshape(-1, 1, 1, 1)\n",
    "print(mean_x)\n",
    "print(std_x)\n",
    "print(mean_y)\n",
    "print(std_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd68131",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_pairs = {\n",
    "    \"All\": [\n",
    "        \"CurveFault_A\",\n",
    "        \"CurveVel_A\",\n",
    "        \"FlatFault_A\",\n",
    "        \"FlatVel_A\",\n",
    "        \"Style_A\",\n",
    "        \"CurveFault_B\",\n",
    "        \"CurveVel_B\",\n",
    "        \"FlatFault_B\",\n",
    "        \"FlatVel_B\",\n",
    "        \"Style_B\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.crosstab(train_paths[\"family\"], train_paths[\"label\"]))\n",
    "display(pd.crosstab(valid_paths[\"family\"], valid_paths[\"label\"]))\n",
    "display(pd.crosstab(test_paths[\"family\"], test_paths[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53820bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "model = LitUNetModel(\n",
    "    model_name=config.model_name,\n",
    "    pretrained=config.pretrained,\n",
    "    num_classes=10,\n",
    "    height=config.height,\n",
    "    width=config.width,\n",
    "    learning_rate=config.learning_rate,\n",
    "    mean_y=mean_y.to(config.device),\n",
    "    std_y=std_y.to(config.device),\n",
    ")\n",
    "\n",
    "# model = torch.compile(model)\n",
    "\n",
    "datamodule = MyDataModule(\n",
    "    train_paths=train_paths,\n",
    "    valid_paths=valid_paths,\n",
    "    test_paths=test_paths,\n",
    "    seed=config.seed,\n",
    "    batch_size=config.batch_size,\n",
    "    height=config.height,\n",
    "    width=config.width,\n",
    "    mean_x=mean_x,\n",
    "    std_x=std_x,\n",
    "    do_transform=config.do_transform,\n",
    ")\n",
    "\n",
    "ema_callback = EMACallback(decay=0.99, ema_device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "callbacks=[\n",
    "    EarlyStopping(monitor=\"val_loss1\", patience=config.patience, mode='min'),\n",
    "    # LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "    TQDMProgressBar(),\n",
    "    ModelCheckpoint(\n",
    "        monitor=\"val_loss1\",\n",
    "        dirpath=config.output_dir,\n",
    "        filename=\"model-{epoch:02d}-{val_loss1:.2f}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "    ema_callback,\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=config.output_dir.joinpath(\"All\"),\n",
    "    enable_checkpointing=True,\n",
    "    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    max_epochs=config.epochs,\n",
    "    precision=\"bf16-mixed\",\n",
    "    callbacks=callbacks,\n",
    "    logger=CSVLogger(config.output_dir, name=\"All\"),\n",
    "    log_every_n_steps=1_000,\n",
    "    val_check_interval=None,\n",
    "    check_val_every_n_epoch=1,\n",
    "    accumulate_grad_batches=config.accumulation_steps,\n",
    "    gradient_clip_val=0,\n",
    "    # benchmark=True,\n",
    ")\n",
    "\n",
    "# training\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "# save last model\n",
    "checkpoint_path = config.output_dir.joinpath(f\"All/multitask_v2_All_{config.seed}.ckpt\")\n",
    "ema_checkpoint_path = config.output_dir.joinpath(\"All/ema_state.pth\")\n",
    "\n",
    "# save check points\n",
    "trainer.save_checkpoint(checkpoint_path)\n",
    "torch.save(ema_callback.ema_state, ema_checkpoint_path)\n",
    "\n",
    "# load the ema state dict\n",
    "ema_state_dict = torch.load(ema_checkpoint_path, map_location=\"cuda\", weights_only=False)\n",
    "model.load_state_dict(ema_state_dict, strict=False)\n",
    "\n",
    "# testing\n",
    "trainer.test(model, datamodule=datamodule)\n",
    "\n",
    "\n",
    "del datamodule\n",
    "del callbacks, ema_callback\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2583a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics0 = pd.read_csv(config.output_dir.joinpath(f\"All/version_0/metrics.csv\"))\n",
    "metrics0 = metrics0.sort_values([\"step\", \"epoch\"]).reset_index(drop=True)\n",
    "train_metric = metrics0[[\"epoch\", \"lr\"]+metrics0.columns[metrics0.columns.str.contains(\"train\")].to_list()].dropna().reset_index(drop=True)\n",
    "valid_metric = metrics0[[\"epoch\"]+metrics0.columns[metrics0.columns.str.contains(\"val\")].to_list()].dropna().reset_index(drop=True)\n",
    "metrics0 = train_metric.merge(valid_metric, on=\"epoch\")\n",
    "display(metrics0.head())\n",
    "\n",
    "_, axs = plt.subplots(6, 1, figsize=(8, 8))\n",
    "metrics0[[\"epoch\", \"lr\"]].dropna().plot(x=\"epoch\", y=\"lr\", kind=\"line\", marker=\".\", ax=axs[0])\n",
    "metrics0[[\"epoch\", \"train_loss\", \"val_loss\"]].dropna().plot(x=\"epoch\", y=[\"train_loss\", \"val_loss\"], kind=\"line\", marker=\".\", ax=axs[1])\n",
    "metrics0[[\"epoch\", \"val_loss1\"]].dropna().plot(x=\"epoch\", y=\"val_loss1\", kind=\"line\", marker=\".\", ax=axs[2])\n",
    "metrics0[[\"epoch\", \"val_loss2\"]].dropna().plot(x=\"epoch\", y=\"val_loss2\", kind=\"line\", marker=\".\", ax=axs[3])\n",
    "metrics0[[\"epoch\", \"val_loss3\"]].dropna().plot(x=\"epoch\", y=\"val_loss3\", kind=\"line\", marker=\".\", ax=axs[4])\n",
    "metrics0[[\"epoch\", \"val_f1\"]].dropna().plot(x=\"epoch\", y=\"val_f1\", kind=\"line\", marker=\".\", ax=axs[5])\n",
    "axs[0].set_xlabel(\"step\")\n",
    "axs[0].set_ylabel(\"learning rate\")\n",
    "\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "axs[2].set_xlabel(\"epoch\")\n",
    "axs[2].set_ylabel(\"Loss1\")\n",
    "\n",
    "axs[3].set_xlabel(\"epoch\")\n",
    "axs[3].set_ylabel(\"Loss2\")\n",
    "\n",
    "axs[4].set_xlabel(\"epoch\")\n",
    "axs[4].set_ylabel(\"Loss3\")\n",
    "\n",
    "axs[5].set_xlabel(\"epoch\")\n",
    "axs[5].set_ylabel(\"F1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7dc7ab",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.log_transform import log_transform_torch\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            paths: List[Path],\n",
    "            height: int,\n",
    "            width: int,\n",
    "            mean_x: torch.Tensor,\n",
    "            std_x: torch.Tensor,\n",
    "        ) -> None:\n",
    "        \n",
    "        self.paths = paths\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.mean_x = mean_x\n",
    "        self.std_x = std_x\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[str, torch.Tensor]:\n",
    "        path = self.paths[index]\n",
    "        file_names = path.stem\n",
    "        x = np.load(path)[\"x\"] # (5, 1000, 70)\n",
    "        x = torch.from_numpy(x)\n",
    "        # x = F.pad(x, pad=(1, 1, 76, 76), mode=\"constant\")\n",
    "        x = log_transform_torch(x)\n",
    "        x = (x - self.mean_x) / self.std_x\n",
    "        x = x.unsqueeze(dim=0) # (1, 5, 1000, 70)\n",
    "        x = F.interpolate(x, size=(self.height, self.width), mode=\"bicubic\")\n",
    "        x = x.squeeze(dim=0) # (5, 1000, 70)\n",
    "        x = x.float()\n",
    "        return file_names, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc564a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.crosstab(test_paths[\"family\"], test_paths[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbac890",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = config.output_dir.joinpath(f\"All/multitask_v2_All_{config.seed}.ckpt\")\n",
    "ema_checkpoint_path = config.output_dir.joinpath(\"All/ema_state.pth\")\n",
    "\n",
    "model = LitUNetModel.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "ema_state_dict = torch.load(ema_checkpoint_path, map_location=\"cuda\", weights_only=False)\n",
    "model.load_state_dict(ema_state_dict, strict=False)\n",
    "\n",
    "test_dataset = InferenceDataset(\n",
    "    paths=list(test_paths[\"path\"]),\n",
    "    height=config.height,\n",
    "    width=config.width,\n",
    "    mean_x=mean_x,\n",
    "    std_x=std_x,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=os.cpu_count()//6,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=\".\",\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    callbacks=[TQDMProgressBar()],\n",
    "    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=\"auto\",\n",
    ")\n",
    "predictions = trainer.predict(model, test_dataloader)\n",
    "test_file_names, test_reg_logits, test_clf_logits = zip(*predictions)\n",
    "test_reg_logits = torch.cat(test_reg_logits)\n",
    "test_clf_logits = torch.cat(test_clf_logits)\n",
    "print(test_reg_logits.shape, test_reg_logits.min(), test_reg_logits.max())\n",
    "\n",
    "random_index = np.random.choice(range(len(test_dataset)), size=5, replace=False)\n",
    "print(random_index)\n",
    "\n",
    "_, axs = plt.subplots(1, 5, figsize=(12, 4))\n",
    "for e, i in enumerate(random_index):\n",
    "    img = axs[e].imshow(test_reg_logits[i, 0], aspect=\"auto\")\n",
    "    plt.colorbar(img, ax=axs[e])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786ecdc",
   "metadata": {},
   "source": [
    "### Compute the confusion matrix and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9090ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths[\"pred_label\"] = torch.argmax(test_clf_logits, dim=1)\n",
    "display(test_paths)\n",
    "\n",
    "cm = multiclass_confusion_matrix(\n",
    "    torch.Tensor(test_paths[\"pred_label\"].values),\n",
    "    torch.Tensor(test_paths[\"label\"].values),\n",
    "    num_classes=10,\n",
    ")\n",
    "display(cm)\n",
    "\n",
    "f1 = multiclass_f1_score(\n",
    "    torch.Tensor(test_paths[\"pred_label\"].values),\n",
    "    torch.Tensor(test_paths[\"label\"].values),\n",
    "    num_classes=10,\n",
    "    average=\"macro\",\n",
    ")\n",
    "display(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1caf163",
   "metadata": {},
   "source": [
    "### Compute the MAE for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ac22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "mae_all = 0\n",
    "for g_name, g in test_paths.reset_index(drop=True).groupby(\"family\"):\n",
    "    print(g_name)\n",
    "    idx = g.index.to_list()\n",
    "    g_reg_images = test_reg_logits[idx]\n",
    "    g_true_images = [np.load(p)[\"y\"] for p in g[\"path\"]]\n",
    "    g_true_images = np.stack(g_true_images, axis=0)\n",
    "    g_true_images = torch.Tensor(g_true_images)\n",
    "    # g_true_images = (g_true_images - statistics[\"All\"][\"mean_y\"]) / statistics[\"All\"][\"std_y\"]\n",
    "    # g_true_images = g_true_images * statistics[\"All\"][\"std_y\"] + statistics[\"All\"][\"mean_y\"]\n",
    "    \n",
    "    _, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    img0 = axs[0].imshow(g_reg_images[0, 0], aspect=\"auto\")\n",
    "    img1 = axs[1].imshow(g_true_images[0, 0], aspect=\"auto\")\n",
    "    img2 = axs[2].imshow(g_true_images[0, 0]-g_reg_images[0, 0], aspect=\"auto\")\n",
    "    plt.colorbar(img0, ax=axs[0])\n",
    "    plt.colorbar(img1, ax=axs[1])\n",
    "    plt.colorbar(img2, ax=axs[2])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    mae = F.l1_loss(\n",
    "        g_reg_images,\n",
    "        g_true_images,\n",
    "        reduction=\"sum\",\n",
    "    )\n",
    "    mae_all += mae.item()\n",
    "    print(f\"MAE: {mae.item() / len(g) / 70 / 70:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "mae_all = mae_all / len(test_paths) / 70 / 70\n",
    "print(f\"All MAE: {mae_all:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6738c95",
   "metadata": {},
   "source": [
    "### Export logs as HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797faedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"jupyter nbconvert --to html --output-dir {config.output_dir} 3_train_multitask_using_AB_v2.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97dad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths.drop(columns=[\"pred_label\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = config.output_dir.joinpath(f\"All/multitask_v2_All_{config.seed}.ckpt\")\n",
    "ema_checkpoint_path = config.output_dir.joinpath(\"All/ema_state.pth\")\n",
    "\n",
    "model = LitUNetModel.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "ema_state_dict = torch.load(ema_checkpoint_path, map_location=\"cuda\", weights_only=False)\n",
    "model.load_state_dict(ema_state_dict, strict=False)\n",
    "\n",
    "datamodule = MyDataModule(\n",
    "    train_paths=train_paths,\n",
    "    valid_paths=valid_paths,\n",
    "    test_paths=test_paths,\n",
    "    seed=config.seed,\n",
    "    batch_size=2*config.batch_size,\n",
    "    height=config.height,\n",
    "    width=config.width,\n",
    "    mean_x=mean_x,\n",
    "    std_x=std_x,\n",
    "    do_transform=False,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=\".\",\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    callbacks=[TQDMProgressBar()],\n",
    "    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=\"auto\",\n",
    ")\n",
    "\n",
    "trainer.test(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9600e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.load(\"../data/FlatFault_A/seis2_1_4_vel2_1_4_364.npz\")\n",
    "x = torch.from_numpy(image[\"x\"]).float()\n",
    "# x = F.pad(x, pad=(1, 1, 76, 76), mode=\"constant\")\n",
    "x = log_transform_torch(x)\n",
    "x = (x - mean_x) / std_x\n",
    "print(x.shape, mean_x.shape, std_x.shape)\n",
    "\n",
    "x = x.unsqueeze(dim=0)\n",
    "x = F.interpolate(x, size=(config.height, config.width), mode=\"bicubic\")\n",
    "x = x.float()\n",
    "\n",
    "y = image[\"y\"].astype(np.float32)\n",
    "\n",
    "x = x.to(config.device)\n",
    "\n",
    "model = model.to(config.device)\n",
    "model.eval()\n",
    "reg, clf = model(x)\n",
    "\n",
    "# reg = reg * statistics[\"All\"][\"std_y\"] + statistics[\"All\"][\"mean_y\"]\n",
    "reg = reg.cpu().detach().numpy()\n",
    "clf = clf.cpu().detach().numpy()\n",
    "print(reg.shape, clf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "img0 = axs[0].imshow(reg[0, 0], aspect=\"auto\")\n",
    "img1 = axs[1].imshow(y[0], aspect=\"auto\")\n",
    "img2 = axs[2].imshow(y[0]-reg[0, 0], aspect=\"auto\")\n",
    "plt.colorbar(img0, ax=axs[0])\n",
    "plt.colorbar(img1, ax=axs[1])\n",
    "plt.colorbar(img2, ax=axs[2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45641c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
